---
title: 'Providing Feedback'
description: 'Help your router learn and improve automatically'
---

## Understanding Feedback

The system improves through:

<CardGroup cols={2}>
  <Card title="Direct Judgements" icon="star">
    * Immediate quality scoring

    * Guide future routing decisions

    * Help identify optimal models
  </Card>

  <Card title="Request/Response Storage" icon="database">
    * Builds interaction history

    * Provides training data

    * Enables performance analysis
  </Card>
</CardGroup>

## Add feedback

To provide direct judgements, let's edit our app.py file to support adding feedback:

```python app.py
from openai import OpenAI
import os
from dotenv import load_dotenv
from config import API_URL, ORG_ID, HEADERS
import requests
import json 

# Load environment variables
load_dotenv()

# Configure OpenAI client
client = OpenAI(
        api_key=os.getenv("MARTIAN_API_KEY"),
        base_url=f"{os.getenv('MARTIAN_API_URL')}/openai/v1"
)

def translate(text):
    """Translate text between Korean and English."""
    request = {"model":"rtr_6c19f40b-6897-4d04-9992-3c05c2e777b3",  # Just change this one line to the name you used when creating a router to use your router
        "messages":[
            {"role": "system", "content": """You are a highly skilled translator with expertise in many languages. Your task is to:
                - Identify if the input is Korean or English
                - If Korean, translate to English
                - If English, translate to Korean
                - Preserve meaning, tone, and nuance
                - Maintain proper grammar and formality level
                """},
            {"role": "user", "content": text}
        ],
        "extra_body":{
           "cost": 0.0005 # The following result will be sent to the best performing model with an expected cost equal or below .005 cents or less
           # "cost": "gpt-4o-mini" # The following result will be sent to the best performing model with an expected cost equal or below gpt-4o-mini price
         }
    }
    response = client.chat.completions.create(
     **request   
    )
    print(response)
    return request, response #.choices[0].message.content.strip()

if __name__ == "__main__":
    while True:
        text = input("\nEnter text to translate (or 'q' to quit): ")
        if text.lower() == 'q':
            break
        router_id = "rtr_6c19f40b-6897-4d04-9992-3c05c2e777b3"
        version_id = "1"
        translation_request, translation_response = translate(text)
        url = f'{API_URL}/org/{ORG_ID}/routers/{router_id}/versions/{version_id}/judgement'
        #print(json.loads(translation_response.json()).keys())
        #exit()
        data = {
            "request": translation_request,
            "response": json.loads(translation_response.json()),
            #'request_response_id': "1123", 
            'judgement': {
                'score': 1.5,
                'rationale': 'The translation was accurate and preserved the meaning of the original text.'
            }
        }

        response = requests.post(url, headers=HEADERS, json=data)
        print(response.json())
        #print(f"\nTranslation: {result}")
```

## Best Practices

1. Provide feedback regularly to improve routing decisions

2. Include detailed rationale for low scores

3. Store important translations for future analysis

4. Use consistent scoring criteria

## How Does Data Persist Across Versions

Feedback data is persisted until the judge is changed.
If you change the judge, the old labels are no longer used when creating new versions of the router.
This is done because new judges can specify dramatically different notions of quality, so the old judgements are then likely to be stale.

## Common Issues

<AccordionGroup>
  <Accordion title="Invalid Score Range">
    Ensure scores are between 0 and 1
  </Accordion>

  <Accordion title="Missing Rationale">
    Always include explanatory rationale, especially for non-perfect scores
  </Accordion>

  <Accordion title="Request Format">
    Verify message format matches OpenAI chat completion format
  </Accordion>
</AccordionGroup>

## Next Steps

After implementing feedback:

1. Monitor router performance

2. Review feedback patterns

3. Track quality improvements

4. Analyze stored translations

<Note>
  Remember to save response IDs for future reference. These can be useful for tracking improvements and analyzing patterns.
</Note>
---
title: "Providing Feedback"
description: "Help your router learn and improve automatically"
---

## Understanding Feedback

The system improves through:

<CardGroup cols="2">
  <Card title="Direct Judgements" icon="star">
    - Immediate quality scoring
    - Guide future routing decisions
    - Help identify optimal models
  </Card>
  <Card title="Request/Response Storage" icon="database">
    - Builds interaction history
    - Provides training data
    - Enables performance analysis
  </Card>
</CardGroup>

## Add feedback

To provide direct judgements, let's edit our app.py file to support adding feedback:

```python app.py
"""Basic translation implementation using OpenAI."""
from openai import OpenAI
import os
from dotenv import load_dotenv
from config import API_URL, ORG_ID, HEADERS
import requests
import json 

# Load environment variables
load_dotenv()

# Configure OpenAI client
API_KEY = os.getenv("MARTIAN_API_KEY")
API_URL = os.getenv("MARTIAN_API_URL")
if not API_KEY:
    raise ValueError("Please set MARTIAN_API_KEY in .env file")
if not API_URL:
    raise ValueError("Please set MARTIAN_API_URL in .env file")
client = OpenAI(
    api_key=API_KEY,
    base_url=f"{API_URL}/openai/v1"
)

ROUTER_ID = "rtr_6c19f40b-6897-4d04-9992-3c05c2e777b3"
ROUTER_VERSION = "1"

def translate(text):
    """Translate text between Korean and English."""
    request = {"model":ROUTER_ID,  # Just change this one line to the name you used when creating a router to use your router
        "messages":[
            {"role": "system", "content": """You are a highly skilled translator with expertise in many languages. Your task is to:
                - Identify if the input is Korean or English
                - If Korean, translate to English
                - If English, translate to Korean
                - Preserve meaning, tone, and nuance
                - Maintain proper grammar and formality level
                """},
            {"role": "user", "content": text}
        ],
        "extra_body":{
           "cost": 0.0005 # The following result will be sent to the best performing model with an expected cost equal or below .005 cents or less
           # "cost": "gpt-4o-mini" # The following result will be sent to the best performing model with an expected cost equal or below gpt-4o-mini price
         }
    }
    response = client.chat.completions.create(
     **request   
    )
    print(response)
    return request, response


def add_request_and_response(router_request, router_response):
    url = f'{API_URL}/org/{ORG_ID}/routers/{ROUTER_ID}/versions/{ROUTER_VERSION}/response'
    data = {
        "request": router_request,
        "response": json.loads(router_response.json())
    }

    response = requests.post(url, headers=HEADERS, json=data)
    return response


def add_feedback(request_response_id, score, rationale=None):
    url = f'{API_URL}/org/{ORG_ID}/routers/{ROUTER_ID}/versions/{ROUTER_VERSION}/judgement'
    data = {
        "request_response_id": request_response_id,  # The ID of the request-response pair
        'judgement': {
            'score': score,  # make sure it's in the judge's score range
            'rationale': rationale or "No rationale provided"
        }
    }
    response = requests.post(url, headers=HEADERS, json=data)
    return response

if __name__ == "__main__":
    while True:
        text = input("\nEnter text to translate (or 'q' to quit): ")
        if text.lower() == 'q':
            break
        translation_request, translation_response = translate(text)
        print(f"\nTranslation: {translation_response.choices[0].message.content.strip()}")

        request_response = add_request_and_response(translation_request, translation_response).json()
        if "detail" in request_response:
            raise ValueError(f"Bad Router id or Router Version, please check - {request_response['detail']}")
        request_response_id = request_response["request_response_id"]
        score = input("\nWhat was the translation quality? 0 (completely inaccurate) to 1 (completely accurate): ")
        add_feedback(request_response_id, score)
```

## Best Practices

1. Provide feedback regularly to improve routing decisions
2. Include detailed rationale for low scores
3. Store important translations for future analysis
4. Use consistent scoring criteria

## How Does Data Persist Across Versions

Feedback data is persisted until the judge is changed.
If you change the judge, the old labels are no longer used when creating new versions of the router.
This is done because new judges can specify dramatically different notions of quality, so the old judgements are then likely to be stale.

## Common Issues

<AccordionGroup>
  <Accordion title="Invalid Score Range">
    Ensure scores are between 0 and 1
  </Accordion>
  <Accordion title="Missing Rationale">
    Always include explanatory rationale, especially for non-perfect scores
  </Accordion>
  <Accordion title="Request Format">
    Verify message format matches OpenAI chat completion format
  </Accordion>
</AccordionGroup>

## Next Steps

After implementing feedback:

1. Monitor router performance
2. Review feedback patterns
3. Track quality improvements
4. Analyze stored translations

<Note>
  Remember to save response IDs for future reference. These can be useful for tracking improvements and analyzing patterns.
</Note>
---
title: "Creating Judges"
description: "Set up quality metrics for your router"
---

## Understanding Judges

Judges are the foundation of Martian's quality optimization system. They:

* Define what "good" means for your specific use case

* Automatically evaluate model outputs

* Guide router decisions for model selection

* Enable continuous quality improvement

<CardGroup cols={2}>
  <Card title="Quality Definition" icon="scale-balanced">
    Define custom quality metrics specific to your translation needs
  </Card>

  <Card title="Automatic Evaluation" icon="robot">
    Automatically score translations based on your criteria
  </Card>
</CardGroup>

## Creating a Translation Judge

Let's create a judge for Korean-English translation:

Start by creating a `create_judge.py` file.

With the following script you can change rubric, possible scores range, and also you can set generate\_validation\_plan to True to get more advanced judge.

```python create_judge.py

from config import API_URL, ORG_ID, HEADERS
import requests

url = f'{API_URL}/org/{ORG_ID}/judges'
data = {
    "name": "Translation Judge",
    "description": "A test judge for translation quality",
    "structure": "RubricNumericalJudge",
    "parameters": {
        "min_score": {"value": 0, "frozen": False},
        "max_score": {"value": 1, "frozen": False},
        "rubric": {"value": "Determine whether the translation is accurate and faithful to the original text.", "frozen": False},
        "judgeLLM": {"value": "openai/openai/gpt-4o-mini", "frozen": False},
    },
    "generate_validation_plan": False,
}
response = requests.post(url, headers=HEADERS, json=data)
print(response.json())

```

## Run the judge

First, let's create some data which we can judge. We need a request and a response. These should be in the same format as produced by running a router via the openai api. For convenience, here are two files with example data:

```json translation_request.json
{
    "model": "rtr_6c19f40b-6897-4d04-9992-3c05c2e777b3",
    "messages": [
        {
            "role": "system",
            "content": "You are a highly skilled translator with expertise in many languages. Your task is to:\n                - Identify if the input is Korean or English\n                - If Korean, translate to English\n                - If English, translate to Korean\n                - Preserve meaning, tone, and nuance\n                - Maintain proper grammar and formality level\n                "
        },
        {
            "role": "user",
            "content": "hello world"
        }
    ],
    "extra_body": {
        "cost": 0.0005
    }
}
```

```json translation_response.json
{
    "id": "8d5f5360-710c-4c7e-a0b5-546b3a0521f5",
    "choices": [
        {
            "finish_reason": "stop",
            "index": 0,
            "logprobs": null,
            "message": {
                "content": "세상에 안녕하세요",
                "refusal": null,
                "role": "assistant",
                "audio": null,
                "function_call": null,
                "tool_calls": null
            }
        }
    ],
    "created": 1738740820,
    "model": "gpt-4-0613",
    "object": "chat.completion",
    "service_tier": "default",
    "system_fingerprint": null,
    "usage": {
        "completion_tokens": 8,
        "prompt_tokens": 81,
        "total_tokens": 89,
        "completion_tokens_details": {
            "accepted_prediction_tokens": 0,
            "audio_tokens": 0,
            "reasoning_tokens": 0,
            "rejected_prediction_tokens": 0
        },
        "prompt_tokens_details": {
            "audio_tokens": 0,
            "cached_tokens": 0
        }
    },
    "cost": 0.00291,
    "response": {
        "role": "assistant",
        "content": "세상에 안녕하세요"
    },
    "token_counts": {
        "prompt": 81.0,
        "completion": 8.0,
        "request": 0.0
    }
}
```

Now we are ready to get evaluations from the judge. After you create a script called `run_judge.py` go ahead and run the code below in that script.

```python run_judge.py
from config import API_URL, ORG_ID, HEADERS
import requests
import json

with open("./translation_request.json") as f:
    translation_request = json.loads(f.read())

with open("./translation_response.json") as f:
    translation_response = json.loads(f.read())


def test_judge(judge_id, version_id):
    url = f'{API_URL}/org/{ORG_ID}/judges/{judge_id}/versions/{version_id}/call'
    
    # Example translation pair
    test_data = {
        'request': translation_request,
        'response': translation_response
    }
    
    response = requests.post(url, headers=HEADERS, json=test_data)
    result = response.json()
    print(result)


judge_id = input("Enter judge ID: ")
version_id = input("Enter version ID: ")
# request_response_id = input("Enter request response ID: ")
test_judge(judge_id, version_id)
```

## Pass judgements to the judge

Start by creating a `add_judgement.py` file.

With the following script you can close the feedback loop and send judgements to the judge.

```python add_judgement.py
import requests
from config import API_URL, ORG_ID, HEADERS, client
import json

judge_id = input("Enter judge ID: ")
version_id = input("Enter version ID: ")

with open("./translation_request.json") as f:
    translation_request = json.loads(f.read())

with open("./translation_response.json") as f:
    translation_response = json.loads(f.read())

judgement = {"score": 1, "rationale": "Very useful"}

url = f'{API_URL}/org/{ORG_ID}/judges/{judge_id}/versions/{version_id}/judgement'

data = {
    'request': translation_request,
    'response': translation_response,
    'judgement': judgement
}

response = requests.post(url, headers=HEADERS, json=data)
print(response.json())
```

## Creating a new router using your judge

Once you feel confident with your judge you can create a new router with this judge. To update an existing router with your judge (see the next section).

Start by creating a `create_router_for_judge.py` file and add the following code.

```python create_router_using_judge.py
from config import API_URL, ORG_ID, HEADERS
import requests

judge_id = input("Enter judge ID: ")
judge_version_id = input("Enter judge version ID: ")

url = f'{API_URL}/org/{ORG_ID}/routers'

data = {
    'name': 'translation-router',
    'description': 'Routes Korean-English translation requests based on quality needs',
    'baseline_model': 'gpt-4o-mini',  # The model you currently use for the application
    'judge_id': judge_id,
    'judge_version_id': judge_version_id,
}

response = requests.post(url, headers=HEADERS, json=data)
print(response.json())
router_id = response.json()['router_id']
print(f"Created translation router with ID: {router_id}")
```

## Update existing router with judge

Start by creating a `update_router.py` file.

With the following script you can update an existing router to use the judge you created.

```python update_router.py
from config import API_URL, ORG_ID, HEADERS
import requests

router_id = input("Enter router ID: ")
router_version_id = input("Enter router version ID: ")
judge_id = input("Enter judge ID: ")
judge_version_id = input("Enter judge version ID: ")

url = f'{API_URL}/org/{ORG_ID}/routers/{router_id}/versions/{router_version_id}'

data = {
    'judge_id': judge_id,
    'judge_version_id': judge_version_id
}

response = requests.post(url, headers=HEADERS, json=data)
print(response.json())
router_id = response.json()['router_id']
print(f"Updated router with ID: {router_id}")
```

## View judge details

Start by creating a `view_judge.py` file.

With the following script you can get the details of the judge object.

```python view_judge.py
from config import API_URL, ORG_ID, HEADERS
import requests

judge_id = input("Enter judge ID: ")

url = f'{API_URL}/org/{ORG_ID}/judges/{judge_id}'
response = requests.get(url, headers=HEADERS)
details = response.json()
print(details)
```

## Monitor judge performance

Start by creating a `evaluate_judge.py` file.

With the following script, you can measure the interannotator agreement of the judge with the ideal outputs, and optionally download that data to csv.

```python evaluate_judge.py
from config import API_URL, ORG_ID, HEADERS
import requests

def evaluate_judge(judge_id, version_id, export_csv=False):
    """Evaluate judge alignment with human judgements."""
    url = f'{API_URL}/org/{ORG_ID}/judges/{judge_id}/versions/{version_id}/evaluate'
    
    # Send export_to_csv directly as the request body
    response = requests.post(url, headers=HEADERS, json=export_csv)
    
    if export_csv:
        # Save CSV response to file
        filename = f'judge_comparison_{judge_id}_{version_id}.csv'
        with open(filename, 'wb') as f:
            f.write(response.content)
        print(f'Results exported to {filename}')
    else:
        # Print JSON results
        result = response.json()
        print(f"IAA Score: {result['iaa_score']}")
        print(f"Number of comparisons: {result['num_comparisons']}")

# Get inputs and run evaluation
judge_id = input("Enter judge ID: ")
version_id = input("Enter version ID (or 'latest'): ")
export_csv = input("Export detailed results to CSV? (y/n): ").lower() == 'y'

evaluate_judge(judge_id, version_id, export_csv)
```

## Best Practices

1. Test judge behavior with known good/bad translations

2. Monitor IAA scores across versions

3. Regularly review judge evaluations

4. Calibrate quality thresholds based on business needs

## Troubleshooting

<AccordionGroup>
  <Accordion title="Inconsistent Scores">
    Check IAA scores and review recent evaluations
  </Accordion>

  <Accordion title="Low Quality Scores">
    Review quality criteria and judge version
  </Accordion>

  <Accordion title="Integration Issues">
    Verify judge ID and version compatibility
  </Accordion>
</AccordionGroup>

<Note>
  Remember to periodically review judge versions to ensure they align with your current quality standards.
</Note>
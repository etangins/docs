---
title: 'Initial Application'
description: 'Start with a basic LLM application'
---

Let's start with a simple translation application. We'll then show how easy it is to optimize it using Martian's routing system.

## Basic Translation App
Here's our starting application:

```python basic_translator.py
from openai import OpenAI
import os
from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Configure OpenAI client
client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

def translate(text):
    """Translate text between Korean and English."""
    response = client.chat.completions.create(
        model="gpt-4",  # Fixed model for all translations
        messages=[
            {"role": "system", "content": """You are a highly skilled translator with expertise in many languages. Your task is to:
                - Identify if the input is Korean or English
                - If Korean, translate to English
                - If English, translate to Korean
                - Preserve meaning, tone, and nuance
                - Maintain proper grammar and formality level
                """},
            {"role": "user", "content": text}
        ]
    )
    return response.choices[0].message.content.strip()

if __name__ == "__main__":
    while True:
        text = input("\nEnter text to translate (or 'q' to quit): ")
        if text.lower() == 'q':
            break
            
        result = translate(text)
        print(f"\nTranslation: {result}")
```

## Current Limitations of Using OpenAI Instead of Martian
This basic implementation works but has some drawbacks:

<CardGroup cols={2}>
  <Card title="Fixed Model" icon="lock">
    Uses the same model regardless of the translation complexity
  </Card>
  <Card title="Cost Inefficient" icon="money-bill">
    May use an expensive model for simple translations
  </Card>
</CardGroup>

Let's see how we can optimize this application with Martian...